{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyP134F6bIShvw7ai3AEqj0N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SmritiReddyy/MLOps_OCR_Only/blob/main/SmallFontFixes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 1: INSTALL ALL DEPENDENCIES\n",
        "# ============================================================\n",
        "# %%capture\n",
        "\n",
        "!pip uninstall -y numpy paddlepaddle paddleocr torch transformers sentencepiece 2>/dev/null\n",
        "!pip install numpy==1.26.4\n",
        "!pip install -q pymupdf Pillow scipy\n",
        "# !pip install -q pytesseract\n",
        "# !apt-get install -qq tesseract-ocr tesseract-ocr-spa > /dev/null 2>&1\n",
        "!pip install paddlepaddle==2.6.2\n",
        "!pip install paddleocr==2.8.1\n",
        "!pip install torch==2.2.2\n",
        "!pip install transformers==4.38.2 sentencepiece\n",
        "!pip install langchain==0.0.352 langchain-community\n",
        "!pip install groq\n",
        "!pip install presidio-analyzer presidio-anonymizer\n",
        "!pip install sacrebleu\n",
        "!python -m spacy download en_core_web_lg -q\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ All dependencies installed\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "56NDgPAO3bez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 2: IMPORTS & GLOBAL CONFIGURATION\n",
        "# ============================================================\n",
        "import os, re, json, time, tempfile\n",
        "import numpy as np\n",
        "import torch\n",
        "import pymupdf\n",
        "from PIL import Image, ImageDraw\n",
        "from scipy.ndimage import sobel\n",
        "from IPython.display import display, Image as IPImage\n",
        "# import pytesseract\n",
        "\n",
        "os.environ[\"PADDLE_PDX_DISABLE_MODEL_SOURCE_CHECK\"] = \"True\"\n",
        "\n",
        "# PaddleOCR\n",
        "PaddleOCR = None\n",
        "try:\n",
        "    from paddleocr import PaddleOCR\n",
        "    print(\"‚úÖ PaddleOCR: available\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  PaddleOCR unavailable: {e} ‚Äî will fall back to Tesseract\")\n",
        "\n",
        "# Global constants\n",
        "OCR_DPI              = 300\n",
        "OCR_CONFIDENCE       = 0.50\n",
        "WORK_DIR             = \"/content/courtaccess_output\"\n",
        "os.makedirs(WORK_DIR, exist_ok=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"\\n‚úÖ Device: {device}\")\n",
        "print(\"‚úÖ All imports ready\")\n",
        "\n"
      ],
      "metadata": {
        "id": "AsaacT6y3Wb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================\n",
        "# CELL 3: UPLOAD PDF\n",
        "# ============================================================\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "\n",
        "print(\"üì§ Upload a PDF to translate:\")\n",
        "try:\n",
        "    uploaded = files.upload()\n",
        "    if uploaded:\n",
        "        fname = list(uploaded.keys())[0]\n",
        "        pdf_path = Path(WORK_DIR) / fname\n",
        "        pdf_path.write_bytes(uploaded[fname])\n",
        "        INPUT_PDF = str(pdf_path)\n",
        "        print(f\"‚úÖ Uploaded: {INPUT_PDF}\")\n",
        "except KeyboardInterrupt:\n",
        "    INPUT_PDF = None\n",
        "    print(\"Upload skipped.\")\n",
        "\n",
        "assert INPUT_PDF and \"courtaccess_translated\" not in INPUT_PDF, \\\n",
        "    \"‚ùå INPUT_PDF points to a previous output ‚Äî re-upload your original PDF.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "qhyUqIRr3fcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================\n",
        "# CELL 4: FONT UTILITIES\n",
        "# Maps PDF font names to PyMuPDF built-in codes.\n",
        "# fit_fontsize() shrinks translated text to fit bounding boxes\n",
        "# ‚Äî critical because Spanish runs ~20-30% longer than English.\n",
        "# ============================================================\n",
        "\n",
        "FONT_MAP = {\n",
        "    \"Helvetica\": \"helv\",           \"Helvetica-Bold\": \"hebo\",\n",
        "    \"Helvetica-Oblique\": \"heit\",   \"Helvetica-BoldOblique\": \"hebi\",\n",
        "    \"Times-Roman\": \"tiro\",         \"Times-Bold\": \"tibo\",\n",
        "    \"Times-Italic\": \"tiit\",        \"Times-BoldItalic\": \"tibi\",\n",
        "    \"Courier\": \"cour\",             \"Courier-Bold\": \"cobo\",\n",
        "    \"Courier-Oblique\": \"coit\",     \"Courier-BoldOblique\": \"cobi\",\n",
        "    \"Symbol\": \"symb\",              \"ZapfDingbats\": \"zadb\",\n",
        "}\n",
        "\n",
        "def get_font_code(span):\n",
        "    fontname = span.get(\"font\", \"\")\n",
        "    flags    = span.get(\"flags\", 0)\n",
        "    direct   = FONT_MAP.get(fontname)\n",
        "    if direct:\n",
        "        return direct\n",
        "    is_bold   = bool(flags & 16) or \"bold\"    in fontname.lower()\n",
        "    is_italic = bool(flags & 2)  or \"italic\"  in fontname.lower() or \"oblique\" in fontname.lower()\n",
        "    is_mono   = bool(flags & 8)  or \"courier\" in fontname.lower() or \"mono\"    in fontname.lower()\n",
        "    is_serif  = bool(flags & 4)  or \"times\"   in fontname.lower() or \"serif\"   in fontname.lower() or \"nimbus\" in fontname.lower()\n",
        "    if is_mono:\n",
        "        return \"cobi\" if is_bold and is_italic else \"cobo\" if is_bold else \"coit\" if is_italic else \"cour\"\n",
        "    elif is_serif:\n",
        "        return \"tibi\" if is_bold and is_italic else \"tibo\" if is_bold else \"tiit\" if is_italic else \"tiro\"\n",
        "    else:\n",
        "        return \"hebi\" if is_bold and is_italic else \"hebo\" if is_bold else \"heit\" if is_italic else \"helv\"\n",
        "\n",
        "def fit_fontsize(text, original_size, bbox_width, fontname=\"helv\", min_size=4.0):\n",
        "    font = pymupdf.Font(fontname)\n",
        "    size = original_size\n",
        "    while size >= min_size:\n",
        "        if font.text_length(text, fontsize=size) <= bbox_width * 0.95:\n",
        "            return size\n",
        "        size -= 0.5\n",
        "    return min_size\n",
        "\n",
        "def estimate_fontsize(text, target_width, fontname=\"helv\"):\n",
        "    font = pymupdf.Font(fontname)\n",
        "    best = 5.0\n",
        "    for s10 in range(50, 260, 2):\n",
        "        size = s10 / 10.0\n",
        "        if font.text_length(text, fontsize=size) <= target_width:\n",
        "            best = size\n",
        "        else:\n",
        "            break\n",
        "    return best\n",
        "\n",
        "def get_background_color(page, bbox):\n",
        "    try:\n",
        "        pix = page.get_pixmap(clip=pymupdf.Rect(bbox), matrix=pymupdf.Matrix(1, 1))\n",
        "        s = pix.samples\n",
        "        if len(s) >= 3:\n",
        "            return (s[0]/255, s[1]/255, s[2]/255)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return (1, 1, 1)\n",
        "\n",
        "print(\"‚úÖ Font utilities ready\")\n"
      ],
      "metadata": {
        "id": "mOQakLTT3lAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 5: PAGE CLASSIFIER (with handwriting detection)\n",
        "# DIGITAL     ‚Äî has vector drawings (borders, boxes, lines)\n",
        "# SCANNED     ‚Äî image-based, no vector drawings\n",
        "# OCR'd SCAN  ‚Äî text spans + embedded image, no drawings\n",
        "# BLANK       ‚Äî no text, no images\n",
        "# ============================================================\n",
        "def classify_page(page):\n",
        "\n",
        "    td = page.get_text(\"dict\", flags=pymupdf.TEXTFLAGS_TEXT)\n",
        "\n",
        "    span_count = 0\n",
        "    for b in td[\"blocks\"]:\n",
        "        if b[\"type\"] == 0:\n",
        "            for line in b[\"lines\"]:\n",
        "                for span in line[\"spans\"]:\n",
        "                    if span[\"text\"].strip():\n",
        "                        span_count += 1\n",
        "\n",
        "    images = page.get_images(full=True)\n",
        "    drawings = page.get_drawings()\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # CRITICAL FIX\n",
        "    # If page has real text, treat it as DIGITAL.\n",
        "    # -------------------------------------------------\n",
        "\n",
        "    if span_count > 15:\n",
        "        return {\n",
        "            \"page_type\": \"DIGITAL\",\n",
        "            \"is_scanned\": False,\n",
        "            \"span_count\": span_count,\n",
        "            \"images\": len(images),\n",
        "            \"drawings\": len(drawings),\n",
        "        }\n",
        "\n",
        "    # If almost no text but has images ‚Üí scanned\n",
        "    if span_count < 5 and images:\n",
        "        return {\n",
        "            \"page_type\": \"SCANNED\",\n",
        "            \"is_scanned\": True,\n",
        "            \"span_count\": span_count,\n",
        "            \"images\": len(images),\n",
        "            \"drawings\": len(drawings),\n",
        "        }\n",
        "\n",
        "    # Fallback\n",
        "    return {\n",
        "        \"page_type\": \"DIGITAL\",\n",
        "        \"is_scanned\": False,\n",
        "        \"span_count\": span_count,\n",
        "        \"images\": len(images),\n",
        "        \"drawings\": len(drawings),\n",
        "    }\n",
        "\n",
        "\n",
        "def is_content_image(info):\n",
        "    return info[\"img_coverage\"] > 80 and info[\"span_count\"] < 5\n",
        "\n",
        "print(\"‚úÖ Page classifier ready\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ZnGTwJ2C4G1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pymupdf\n",
        "\n",
        "doc = pymupdf.open(\"newtest.pdf\")  # replace with your actual filename\n",
        "page = doc[0]\n",
        "td = page.get_text(\"dict\")\n",
        "blocks = [b for b in td[\"blocks\"] if b[\"type\"] == 0]\n",
        "print(f\"Total blocks: {len(blocks)}\")\n",
        "for i, b in enumerate(blocks):\n",
        "    first_text = b[\"lines\"][0][\"spans\"][0][\"text\"][:60]\n",
        "    bbox = b[\"bbox\"]\n",
        "    print(f\"Block {i}: bbox={bbox} | text='{first_text}'\")"
      ],
      "metadata": {
        "id": "VA8CFsOT-Vj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 6: NLLB-200 1.3B TRANSLATION SETUP\n",
        "# Loads facebook/nllb-200-distilled-1.3B for English ‚Üí Spanish.\n",
        "# should_translate() skips numbers, citations, URLs, dates, etc.\n",
        "# Hallucination guard rejects outputs with abnormal length ratios.\n",
        "# ============================================================\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "MODEL_NAME  = \"facebook/nllb-200-distilled-1.3B\"\n",
        "SOURCE_LANG = \"eng_Latn\"\n",
        "TARGET_LANG = \"spa_Latn\"\n",
        "\n",
        "print(\"‚è≥ Loading NLLB-200 1.3B...\")\n",
        "nllb_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "nllb_model     = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)\n",
        "nllb_model.eval()\n",
        "print(\"‚úÖ NLLB-200 1.3B ready\")\n",
        "\n",
        "SKIP_PATTERNS = [\n",
        "    r\"^[\\d\\s\\.\\-\\/\\(\\)\\\\,]+$\",\n",
        "    r\"^\\s*[xX]\\s*$\",\n",
        "    r\"^https?://\",\n",
        "    r\"^\\$[\\d,\\.]+$\",\n",
        "    r\"^[\\d]{1,2}[\\/\\-][\\d]{1,2}[\\/\\-][\\d]{2,4}$\",\n",
        "    r\"^[\\-\\s]*$\",\n",
        "    r\"^\\[[\\d\\.\\(\\)a-zA-Z\\;\\s]+\\]\",\n",
        "    r\"^\\d+\\.\\s*$\",\n",
        "    r\"^[_#\\s]+$\",\n",
        "    r\"^Mass\\.\\s+R\\.\",\n",
        "    r\"^\\d+:\\d+-[a-zA-Z]+-\\d+\",\n",
        "]\n",
        "\n",
        "def should_translate(text: str) -> bool:\n",
        "    t = text.strip()\n",
        "    if not t or len(t) <= 3:\n",
        "        return False\n",
        "    if t.isupper() and len(t) < 4:\n",
        "        return False\n",
        "    return not any(re.match(p, t) for p in SKIP_PATTERNS)\n",
        "\n",
        "def split_into_sentences(text: str) -> list:\n",
        "    return [p for p in re.split(r\"(?<=[.!?])\\s+\", text.strip()) if p.strip()]\n",
        "\n",
        "def _raw_batch_translate(texts: list) -> list:\n",
        "    nllb_tokenizer.src_lang = SOURCE_LANG\n",
        "    enc = nllb_tokenizer(\n",
        "        texts, return_tensors=\"pt\", padding=True,\n",
        "        truncation=True, max_length=512\n",
        "    ).to(device)\n",
        "    gen = nllb_model.generate(\n",
        "        **enc,\n",
        "        forced_bos_token_id=nllb_tokenizer.convert_tokens_to_ids(TARGET_LANG),\n",
        "        max_length=min(512, enc[\"input_ids\"].shape[1] + 40),\n",
        "        num_beams=4,\n",
        "        early_stopping=True,\n",
        "    )\n",
        "    return nllb_tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
        "\n",
        "def batch_translate(texts: list) -> list:\n",
        "    results = []\n",
        "    for text in texts:\n",
        "        if not should_translate(text):\n",
        "            results.append(text)\n",
        "            continue\n",
        "        sentences  = split_into_sentences(text)\n",
        "        translated = _raw_batch_translate([text])[0] if len(sentences) <= 1 \\\n",
        "                     else \" \".join(_raw_batch_translate(sentences))\n",
        "        ratio = len(translated) / max(len(text), 1)\n",
        "        if ratio > 2.8 or ratio < 0.15:\n",
        "            print(f\"   ‚ö†Ô∏è  Hallucination (ratio={ratio:.1f}), keeping original: '{text[:60]}'\")\n",
        "            results.append(text)\n",
        "        else:\n",
        "            results.append(translated)\n",
        "    return results\n",
        "\n",
        "print(\"‚úÖ Translation functions ready\")\n"
      ],
      "metadata": {
        "id": "1_UTV03q4JSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 7: GROQ LLaMA 3.3 70B LEGAL VERIFICATION\n",
        "# High-risk spans (legal citations, terms, section references)\n",
        "# are batched and sent to Groq for accuracy review.\n",
        "# Rate-limited to 25 requests/minute (Groq free tier).\n",
        "# ============================================================\n",
        "from groq import Groq\n",
        "from google.colab import userdata\n",
        "\n",
        "groq_client         = Groq(api_key=userdata.get(\"GROQ_API_KEY\"))\n",
        "_groq_request_times = []\n",
        "\n",
        "def _wait_for_rate_limit(max_per_minute=25):\n",
        "    global _groq_request_times\n",
        "    now = time.time()\n",
        "    _groq_request_times = [t for t in _groq_request_times if now - t < 60]\n",
        "    if len(_groq_request_times) >= max_per_minute:\n",
        "        wait = 60 - (now - _groq_request_times[0]) + 1\n",
        "        print(f\"   ‚è≥ Groq rate limit ‚Äî waiting {wait:.1f}s...\")\n",
        "        time.sleep(wait)\n",
        "    _groq_request_times.append(time.time())\n",
        "\n",
        "def _strip_fences(text: str) -> str:\n",
        "    text = text.strip()\n",
        "    for prefix in (\"```json\", \"```\"):\n",
        "        if text.startswith(prefix):\n",
        "            text = text[len(prefix):]\n",
        "    if text.endswith(\"```\"):\n",
        "        text = text[:-3]\n",
        "    return text.strip()\n",
        "\n",
        "HIGH_RISK_PATTERNS = [\n",
        "    r\"Mass\\.\\s+R\\.\",        r\"\\[[\\d\\.\\(\\)a-zA-Z\\;\\s]+\\]\",\n",
        "    r\"\\bCommonwealth\\b\",    r\"\\bwaiver\\b|\\bwaive\\b\",\n",
        "    r\"\\bdefendant\\b|\\bplaintiff\\b\", r\"\\bdocket\\b\",\n",
        "    r\"G\\.L\\.\\s+c\\.\",        r\"Paragraph\\s+\\d+\",\n",
        "    r\"¬ß\\s*\\d+\",             r\"^\\d+\\.\",\n",
        "    r\"\\breciprocal\\b\",      r\"\\bautomatic\\b\",\n",
        "    r\"\\bARRAIGNMENT\\b\",     r\"\\bplea\\b|\\bplead\\b\",\n",
        "]\n",
        "\n",
        "def is_high_risk(text: str) -> bool:\n",
        "    return any(re.search(p, text, re.IGNORECASE) for p in HIGH_RISK_PATTERNS)\n",
        "\n",
        "def verify_page_translations(original_spans: list, translated_spans: list, batch_size=8) -> list:\n",
        "    verified = list(translated_spans)\n",
        "    hi_risk  = [i for i, o in enumerate(original_spans) if should_translate(o) and is_high_risk(o)]\n",
        "    if not hi_risk:\n",
        "        print(\"   ‚úÖ No high-risk spans ‚Äî skipping Groq\")\n",
        "        return verified\n",
        "    print(f\"   üîç {len(hi_risk)}/{len(original_spans)} spans flagged as high-risk\")\n",
        "    for start in range(0, len(hi_risk), batch_size):\n",
        "        batch_idx   = hi_risk[start:start + batch_size]\n",
        "        orig_batch  = [original_spans[i]   for i in batch_idx]\n",
        "        trans_batch = [translated_spans[i] for i in batch_idx]\n",
        "        _wait_for_rate_limit()\n",
        "        pairs = \"\\n\".join(\n",
        "            f\"{k+1}. EN: {o}\\n   ES: {t}\"\n",
        "            for k, (o, t) in enumerate(zip(orig_batch, trans_batch))\n",
        "        )\n",
        "        prompt = (\n",
        "              \"You are a certified legal translator specializing in English to Spanish court document translation. \"\n",
        "              \"For each pair below, verify and correct the Spanish translation. \"\n",
        "              \"Apply standard legal Spanish conventions: use established legal terminology, \"\n",
        "              \"preserve all monetary amounts, dates, proper nouns, case numbers, and legal citations exactly as-is. \"\n",
        "              \"If a term has a standard legal Spanish equivalent, use it. \"\n",
        "              \"If the translation is already accurate, return it unchanged.\\n\\n\"\n",
        "              \"Return ONLY a JSON array of corrected Spanish strings, one per pair, \"\n",
        "              \"in the same order. No explanation, no markdown, just the JSON array.\\n\\n\"\n",
        "              f\"PAIRS:\\n{pairs}\\n\\n\"\n",
        "              'Return format: [\"corrected span 1\", \"corrected span 2\", ...]'\n",
        "          )\n",
        "        try:\n",
        "            resp = groq_client.chat.completions.create(\n",
        "                model=\"llama-3.3-70b-versatile\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.1,\n",
        "                max_tokens=1024,\n",
        "            )\n",
        "            corrected = json.loads(_strip_fences(resp.choices[0].message.content))\n",
        "            if isinstance(corrected, list) and len(corrected) == len(batch_idx):\n",
        "                for idx, c in zip(batch_idx, corrected):\n",
        "                    if c and c.strip():\n",
        "                        ratio = len(c) / max(len(translated_spans[idx]), 1)\n",
        "                        if 0.2 <= ratio <= 4.0:\n",
        "                            verified[idx] = c.strip()\n",
        "                print(f\"   ‚úÖ Batch {start//batch_size + 1}: verified {len(batch_idx)} spans\")\n",
        "            else:\n",
        "                print(f\"   ‚ö†Ô∏è  Batch {start//batch_size + 1}: length mismatch, keeping NLLB output\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è  Groq error: {e} ‚Äî keeping NLLB output\")\n",
        "    return verified\n",
        "\n",
        "# Connection test\n",
        "print(\"üß™ Testing Groq connection...\")\n",
        "_wait_for_rate_limit()\n",
        "_test = groq_client.chat.completions.create(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Reply with just: OK\"}],\n",
        "    max_tokens=5,\n",
        ")\n",
        "print(f\"   Response: {_test.choices[0].message.content.strip()}\")\n",
        "print(\"‚úÖ Groq LLaMA 3.3 70B ready\")\n",
        "\n"
      ],
      "metadata": {
        "id": "AWwjw1-H4M0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 8: PRESIDIO PII REDACTION\n",
        "# Detects and redacts PII in text spans and PDF form widgets.\n",
        "# Legal citations and section references are always preserved.\n",
        "# Set REDACT_PII = False for personal-use translation.\n",
        "# ============================================================\n",
        "from presidio_analyzer import AnalyzerEngine\n",
        "from presidio_anonymizer import AnonymizerEngine\n",
        "from presidio_anonymizer.entities import OperatorConfig\n",
        "\n",
        "print(\"‚è≥ Loading Presidio...\")\n",
        "analyzer   = AnalyzerEngine()\n",
        "anonymizer = AnonymizerEngine()\n",
        "print(\"‚úÖ Presidio ready\")\n",
        "\n",
        "REDACT_PII   = True   # set False for personal use\n",
        "REDACT_DATES = False  # keep dates visible in output\n",
        "\n",
        "PII_ENTITIES = [\n",
        "    \"PERSON\", \"PHONE_NUMBER\", \"EMAIL_ADDRESS\", \"US_SSN\",\n",
        "    \"CREDIT_CARD\", \"US_PASSPORT\", \"US_DRIVER_LICENSE\",\n",
        "    \"IP_ADDRESS\", \"DATE_TIME\",\n",
        "]\n",
        "if not REDACT_DATES and \"DATE_TIME\" in PII_ENTITIES:\n",
        "    PII_ENTITIES.remove(\"DATE_TIME\")\n",
        "\n",
        "PRESERVE_PATTERNS = [\n",
        "    r\"G\\.L\\.\\s+c\\.\\s+\\d+\",    r\"¬ß\\s*\\d+[A-Z]?\",\n",
        "    r\"\\[\\d+[\\.\\(\\)a-zA-Z\\;]+\\]\", r\"Mass\\.\\s+R\\.\\s+Crim\\.\",\n",
        "    r\"Paragraph\\s+\\d+\",         r\"TC\\d+\",\n",
        "    r\"P\\.\\s*\\d+\\.\\d+\",\n",
        "]\n",
        "\n",
        "def _should_preserve(text: str) -> bool:\n",
        "    return any(re.search(p, text, re.IGNORECASE) for p in PRESERVE_PATTERNS)\n",
        "\n",
        "def redact_pii(text: str) -> tuple:\n",
        "    if not REDACT_PII or not text.strip() or _should_preserve(text):\n",
        "        return text, []\n",
        "    try:\n",
        "        results = analyzer.analyze(text=text, entities=PII_ENTITIES, language=\"en\")\n",
        "    except Exception:\n",
        "        return text, []\n",
        "    if not results:\n",
        "        return text, []\n",
        "    found = list(set(r.entity_type for r in results))\n",
        "    anon  = anonymizer.anonymize(\n",
        "        text=text, analyzer_results=results,\n",
        "        operators={e: OperatorConfig(\"replace\", {\"new_value\": f\"[{e}]\"}) for e in found}\n",
        "    )\n",
        "    return anon.text, found\n",
        "\n",
        "def redact_spans(original_texts: list, translated_texts: list) -> list:\n",
        "    if not REDACT_PII:\n",
        "        return translated_texts\n",
        "    redacted = list(translated_texts)\n",
        "    total    = 0\n",
        "    for i, (orig, trans) in enumerate(zip(original_texts, translated_texts)):\n",
        "        _, entities = redact_pii(orig)\n",
        "        if not entities:\n",
        "            continue\n",
        "        redacted[i], _ = redact_pii(trans)\n",
        "        total += 1\n",
        "    if total == 0:\n",
        "        print(f\"   ‚úÖ No PII found in {len(original_texts)} spans\")\n",
        "    else:\n",
        "        print(f\"   üîí Redacted PII in {total}/{len(original_texts)} spans\")\n",
        "    return redacted\n",
        "\n",
        "def redact_form_fields(page) -> int:\n",
        "    if not REDACT_PII:\n",
        "        return 0\n",
        "    count = 0\n",
        "    for w in list(page.widgets()):\n",
        "        val = str(w.field_value or \"\").strip()\n",
        "        if not val:\n",
        "            continue\n",
        "        redacted, entities = redact_pii(val)\n",
        "        if entities:\n",
        "            w.field_value = redacted\n",
        "            w.update()\n",
        "            if w.rect:\n",
        "                page.add_redact_annot(w.rect, fill=(1, 1, 1))\n",
        "            count += 1\n",
        "    if count > 0:\n",
        "        page.apply_redactions(images=pymupdf.PDF_REDACT_IMAGE_NONE)\n",
        "    return count\n",
        "\n",
        "print(\"‚úÖ PII redaction ready\")\n",
        "\n"
      ],
      "metadata": {
        "id": "SzaYKPPK4P59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 9 ‚Äî GENERALIZED LAYOUT-PRESERVING TRANSLATION\n",
        "# Handles: digital forms, scanned docs, mixed handwriting+print\n",
        "# Key fixes:\n",
        "#   - Checkbox/form field text stays positionally anchored\n",
        "#   - Blank/signature lines preserved (they're drawings, not text)\n",
        "#   - Bullet/indented lists not merged into paragraphs\n",
        "#   - Scanned pages sorted by reading order (y then x)\n",
        "#   - Confidence threshold tuning for mixed handwriting+print\n",
        "# ============================================================\n",
        "import pymupdf\n",
        "\n",
        "# ‚îÄ‚îÄ Shared: wrap and insert text into a rect ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "def _wrap_and_insert(page, text, rect, base_size, fontname=\"helv\",\n",
        "                     color=(0, 0, 0), min_size=5.0, line_spacing=1.2):\n",
        "    font = pymupdf.Font(fontname)\n",
        "    words = text.split()\n",
        "    if not words:\n",
        "        return\n",
        "\n",
        "    def wrap_lines(size):\n",
        "        lines, current = [], \"\"\n",
        "        for w in words:\n",
        "            test = (current + \" \" + w).strip()\n",
        "            if font.text_length(test, fontsize=size) <= rect.width:\n",
        "                current = test\n",
        "            else:\n",
        "                if current:\n",
        "                    lines.append(current)\n",
        "                current = w\n",
        "        if current:\n",
        "            lines.append(current)\n",
        "        return lines\n",
        "\n",
        "    chosen_size = base_size\n",
        "    lines = wrap_lines(chosen_size)\n",
        "    while (len(lines) * chosen_size * line_spacing > rect.height) and chosen_size > min_size:\n",
        "        chosen_size -= 0.5\n",
        "        lines = wrap_lines(chosen_size)\n",
        "\n",
        "    y = rect.y0 + chosen_size\n",
        "    for line in lines:\n",
        "        page.insert_text(\n",
        "            pymupdf.Point(rect.x0, y),\n",
        "            line, fontsize=chosen_size,\n",
        "            fontname=fontname, color=color\n",
        "        )\n",
        "        y += chosen_size * line_spacing\n",
        "\n",
        "\n",
        "def _compute_wrap_height(text, size, width, fontname, line_spacing=1.2):\n",
        "    font = pymupdf.Font(fontname)\n",
        "    words = text.split()\n",
        "    if not words:\n",
        "        return size\n",
        "    lines, current = [], \"\"\n",
        "    for w in words:\n",
        "        test = (current + \" \" + w).strip()\n",
        "        if font.text_length(test, fontsize=size) <= width:\n",
        "            current = test\n",
        "        else:\n",
        "            if current:\n",
        "                lines.append(current)\n",
        "            current = w\n",
        "    if current:\n",
        "        lines.append(current)\n",
        "    return max(len(lines), 1) * size * line_spacing\n",
        "\n",
        "\n",
        "# ‚îÄ‚îÄ Detect if a span is likely a form label (short, near a widget) ‚îÄ‚îÄ\n",
        "def _is_form_label(span, page_width):\n",
        "    \"\"\"\n",
        "    Short uppercase text or very short text near left/right edge\n",
        "    is likely a form field label ‚Äî keep it positionally anchored,\n",
        "    don't merge with neighbors.\n",
        "    \"\"\"\n",
        "    text = span[\"text\"].strip()\n",
        "    if not text:\n",
        "        return False\n",
        "    # Short all-caps labels like \"DATE\", \"DOCKET NO.\", \"SIGNATURE OF JUDGE\"\n",
        "    if len(text) < 40 and (text.isupper() or text.endswith(\":\")):\n",
        "        return True\n",
        "    # Very short text (1-3 words) near page edges = likely a label\n",
        "    if len(text.split()) <= 3 and (span[\"bbox\"][0] < 80 or span[\"bbox\"][2] > page_width - 80):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "# ‚îÄ‚îÄ Extract line units with smart merging ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "def _extract_line_units(page, y_tolerance=3.0):\n",
        "    \"\"\"\n",
        "    Extracts translation units from a digital page.\n",
        "    - Groups spans on the same visual line (same y0 ¬± y_tolerance)\n",
        "    - Merges consecutive lines into paragraphs ONLY when:\n",
        "        * Same left margin (x_tolerance)\n",
        "        * Small vertical gap (line_gap_tolerance)\n",
        "        * Neither line looks like a form label\n",
        "        * Neither line is an indented list item relative to the other\n",
        "    - Keeps checkbox labels, field labels, and list items positionally anchored\n",
        "    \"\"\"\n",
        "    td = page.get_text(\"dict\", flags=pymupdf.TEXTFLAGS_TEXT)\n",
        "    page_width = page.rect.width\n",
        "\n",
        "    # Collect all non-empty spans\n",
        "    all_spans = []\n",
        "    for blk in td[\"blocks\"]:\n",
        "        if blk[\"type\"] != 0:\n",
        "            continue\n",
        "        for line in blk[\"lines\"]:\n",
        "            for span in line[\"spans\"]:\n",
        "                if span[\"text\"].strip():\n",
        "                    all_spans.append(span)\n",
        "\n",
        "    if not all_spans:\n",
        "        return []\n",
        "\n",
        "    # Sort top-to-bottom, left-to-right\n",
        "    all_spans.sort(key=lambda s: (round(s[\"bbox\"][1], 1), s[\"bbox\"][0]))\n",
        "\n",
        "    # Group spans sharing the same y0 into visual lines\n",
        "    y_groups = []\n",
        "    current_group = [all_spans[0]]\n",
        "    current_y = all_spans[0][\"bbox\"][1]\n",
        "\n",
        "    for span in all_spans[1:]:\n",
        "        if abs(span[\"bbox\"][1] - current_y) <= y_tolerance:\n",
        "            current_group.append(span)\n",
        "        else:\n",
        "            y_groups.append(current_group)\n",
        "            current_group = [span]\n",
        "            current_y = span[\"bbox\"][1]\n",
        "    y_groups.append(current_group)\n",
        "\n",
        "    # Build line-level units\n",
        "    line_units = []\n",
        "    for group in y_groups:\n",
        "        full_text = \" \".join(s[\"text\"].strip() for s in group)\n",
        "        group_rect = pymupdf.Rect(group[0][\"bbox\"])\n",
        "        for s in group[1:]:\n",
        "            group_rect = group_rect | pymupdf.Rect(s[\"bbox\"])\n",
        "        line_units.append({\n",
        "            \"text\": full_text,\n",
        "            \"rect\": group_rect,\n",
        "            \"first_span\": group[0],\n",
        "            \"is_form_label\": any(_is_form_label(s, page_width) for s in group),\n",
        "            \"x0\": group[0][\"bbox\"][0],  # leftmost x of first span in group\n",
        "        })\n",
        "\n",
        "    # ‚îÄ‚îÄ Merge consecutive lines into paragraphs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # Only merge when:\n",
        "    #   1. Same left margin (x_tolerance)\n",
        "    #   2. Small vertical gap (line_gap_tolerance)\n",
        "    #   3. Neither is a form label\n",
        "    #   4. Not an indented list item (x0 differs significantly = different indent)\n",
        "    x_tolerance        = 5.0   # pts ‚Äî must share same left margin to merge\n",
        "    line_gap_tolerance = 4.0   # pts ‚Äî max gap between consecutive lines\n",
        "    indent_threshold   = 8.0   # pts ‚Äî x0 diff larger than this = different indent level\n",
        "\n",
        "    units = []\n",
        "    if not line_units:\n",
        "        return units\n",
        "\n",
        "    current = dict(line_units[0])\n",
        "\n",
        "    for nxt in line_units[1:]:\n",
        "        cur_bottom  = current[\"rect\"].y1\n",
        "        next_top    = nxt[\"rect\"].y0\n",
        "        gap         = next_top - cur_bottom\n",
        "        same_margin = abs(current[\"x0\"] - nxt[\"x0\"]) <= x_tolerance\n",
        "        close_lines = 0 <= gap <= line_gap_tolerance\n",
        "        neither_label = not current[\"is_form_label\"] and not nxt[\"is_form_label\"]\n",
        "        same_indent = abs(current[\"x0\"] - nxt[\"x0\"]) <= indent_threshold\n",
        "\n",
        "        if same_margin and close_lines and neither_label and same_indent:\n",
        "            current[\"text\"] = current[\"text\"].rstrip() + \" \" + nxt[\"text\"].lstrip()\n",
        "            current[\"rect\"] = current[\"rect\"] | nxt[\"rect\"]\n",
        "        else:\n",
        "            units.append(current)\n",
        "            current = dict(nxt)\n",
        "\n",
        "    units.append(current)\n",
        "    return units\n",
        "\n",
        "\n",
        "# ‚îÄ‚îÄ DIGITAL page reconstruction ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "def reconstruct_digital_page(out_page, use_verification=True):\n",
        "    units = _extract_line_units(out_page)\n",
        "    if not units:\n",
        "        return 0\n",
        "\n",
        "    original_texts   = [u[\"text\"] for u in units]\n",
        "    translated_texts = batch_translate(original_texts)\n",
        "\n",
        "    if use_verification:\n",
        "        translated_texts = verify_page_translations(\n",
        "            original_texts, translated_texts, batch_size=8\n",
        "        )\n",
        "\n",
        "    translated_texts = redact_spans(original_texts, translated_texts)\n",
        "\n",
        "    # Redact all original text\n",
        "    td = out_page.get_text(\"dict\", flags=pymupdf.TEXTFLAGS_TEXT)\n",
        "    for blk in td[\"blocks\"]:\n",
        "        if blk[\"type\"] != 0:\n",
        "            continue\n",
        "        for line in blk[\"lines\"]:\n",
        "            for span in line[\"spans\"]:\n",
        "                if span[\"text\"].strip():\n",
        "                    out_page.add_redact_annot(\n",
        "                        pymupdf.Rect(span[\"bbox\"]),\n",
        "                        fill=(1, 1, 1)\n",
        "                    )\n",
        "    out_page.apply_redactions(images=pymupdf.PDF_REDACT_IMAGE_NONE)\n",
        "\n",
        "    count = 0\n",
        "    prev_bottom = 0.0\n",
        "\n",
        "    for unit, translated_text in zip(units, translated_texts):\n",
        "        first_span = unit[\"first_span\"]\n",
        "        font  = get_font_code(first_span)\n",
        "        color = pymupdf.sRGB_to_rgb(first_span[\"color\"])\n",
        "        size  = max(first_span[\"size\"] - 2.0, 5.0)\n",
        "\n",
        "        exact_height = _compute_wrap_height(\n",
        "            translated_text, size, unit[\"rect\"].width, font\n",
        "        )\n",
        "\n",
        "        # For form labels: preserve original x/y exactly ‚Äî don't nudge\n",
        "        if unit[\"is_form_label\"]:\n",
        "            y0 = unit[\"rect\"].y0\n",
        "        else:\n",
        "            natural_y0 = unit[\"rect\"].y0\n",
        "            y0 = natural_y0 if natural_y0 >= prev_bottom else prev_bottom + 1.0\n",
        "\n",
        "        rect = pymupdf.Rect(\n",
        "            unit[\"rect\"].x0, y0,\n",
        "            unit[\"rect\"].x1, y0 + exact_height\n",
        "        )\n",
        "\n",
        "        natural_bottom = unit[\"rect\"].y1\n",
        "        prev_bottom = max(rect.y1, natural_bottom)\n",
        "\n",
        "        _wrap_and_insert(out_page, translated_text, rect, size, font, color)\n",
        "        count += 1\n",
        "\n",
        "    return count\n",
        "\n",
        "\n",
        "# ‚îÄ‚îÄ SCANNED page reconstruction (PaddleOCR) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "def reconstruct_scanned_page_paddle(out_page, paddle_engine, use_verification=True):\n",
        "    mat = pymupdf.Matrix(300 / 72, 300 / 72)\n",
        "    pix = out_page.get_pixmap(matrix=mat)\n",
        "    img_path = os.path.join(WORK_DIR, f\"_ocr_tmp_{out_page.number}.png\")\n",
        "    pix.save(img_path)\n",
        "\n",
        "    try:\n",
        "        ocr_result = paddle_engine.ocr(img_path, cls=True)\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è  PaddleOCR failed: {e}\")\n",
        "        return 0\n",
        "\n",
        "    if not ocr_result or not ocr_result[0]:\n",
        "        return 0\n",
        "\n",
        "    scale_x = out_page.rect.width  / pix.width\n",
        "    scale_y = out_page.rect.height / pix.height\n",
        "\n",
        "    # Collect all OCR regions\n",
        "    raw_units = []\n",
        "    for line in ocr_result[0]:\n",
        "        box, (text, conf) = line[0], line[1]\n",
        "        if conf < OCR_CONFIDENCE or not text.strip():\n",
        "            continue\n",
        "        xs = [pt[0] * scale_x for pt in box]\n",
        "        ys = [pt[1] * scale_y for pt in box]\n",
        "        rect = pymupdf.Rect(min(xs), min(ys), max(xs), max(ys))\n",
        "        raw_units.append({\"text\": text, \"rect\": rect, \"conf\": conf})\n",
        "\n",
        "    if not raw_units:\n",
        "        return 0\n",
        "\n",
        "    # FIX: Sort by reading order (top-to-bottom, left-to-right)\n",
        "    # Group into rows by y-proximity, then sort each row left-to-right\n",
        "    raw_units.sort(key=lambda u: (u[\"rect\"].y0, u[\"rect\"].x0))\n",
        "    row_gap = 15.0  # pts ‚Äî vertical gap to consider a new row\n",
        "    rows = []\n",
        "    current_row = [raw_units[0]]\n",
        "    current_y = raw_units[0][\"rect\"].y0\n",
        "\n",
        "    for u in raw_units[1:]:\n",
        "        if abs(u[\"rect\"].y0 - current_y) <= row_gap:\n",
        "            current_row.append(u)\n",
        "        else:\n",
        "            current_row.sort(key=lambda x: x[\"rect\"].x0)\n",
        "            rows.append(current_row)\n",
        "            current_row = [u]\n",
        "            current_y = u[\"rect\"].y0\n",
        "    current_row.sort(key=lambda x: x[\"rect\"].x0)\n",
        "    rows.append(current_row)\n",
        "\n",
        "    units = [u for row in rows for u in row]\n",
        "\n",
        "    original_texts   = [u[\"text\"] for u in units]\n",
        "    translated_texts = batch_translate(original_texts)\n",
        "\n",
        "    if use_verification:\n",
        "        translated_texts = verify_page_translations(\n",
        "            original_texts, translated_texts, batch_size=8\n",
        "        )\n",
        "\n",
        "    translated_texts = redact_spans(original_texts, translated_texts)\n",
        "\n",
        "    count = 0\n",
        "    for unit, translated_text in zip(units, translated_texts):\n",
        "        rect = unit[\"rect\"]\n",
        "        size = max(rect.height * 0.65 - 2.0, 5.0)\n",
        "\n",
        "        out_page.add_redact_annot(rect, fill=(1, 1, 1))\n",
        "        out_page.apply_redactions(images=pymupdf.PDF_REDACT_IMAGE_NONE)\n",
        "\n",
        "        exact_height = _compute_wrap_height(translated_text, size, rect.width, \"helv\")\n",
        "        expanded_rect = pymupdf.Rect(\n",
        "            rect.x0, rect.y0,\n",
        "            rect.x1, rect.y0 + exact_height\n",
        "        )\n",
        "\n",
        "        _wrap_and_insert(out_page, translated_text, expanded_rect,\n",
        "                         base_size=size, fontname=\"helv\", color=(0, 0, 0))\n",
        "        count += 1\n",
        "\n",
        "    return count\n",
        "\n",
        "\n",
        "# ‚îÄ‚îÄ SCANNED page reconstruction (Tesseract fallback) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "def reconstruct_scanned_page_tesseract(out_page, use_verification=True):\n",
        "    import pytesseract\n",
        "    from PIL import Image as PILImage\n",
        "\n",
        "    mat = pymupdf.Matrix(300 / 72, 300 / 72)\n",
        "    pix = out_page.get_pixmap(matrix=mat)\n",
        "    img = PILImage.frombytes(\"RGB\", (pix.width, pix.height), pix.samples)\n",
        "\n",
        "    scale_x = out_page.rect.width  / pix.width\n",
        "    scale_y = out_page.rect.height / pix.height\n",
        "\n",
        "    data = pytesseract.image_to_data(\n",
        "        img, lang=\"eng\",\n",
        "        config=\"--psm 6\",  # assume uniform block of text ‚Äî better reading order\n",
        "        output_type=pytesseract.Output.DICT\n",
        "    )\n",
        "\n",
        "    # Group words into lines by (block_num, par_num, line_num)\n",
        "    line_map = {}\n",
        "    for i, word in enumerate(data[\"text\"]):\n",
        "        if not word.strip():\n",
        "            continue\n",
        "        conf = int(data[\"conf\"][i])\n",
        "        if conf < 30:\n",
        "            continue\n",
        "        key = (data[\"block_num\"][i], data[\"par_num\"][i], data[\"line_num\"][i])\n",
        "        line_map.setdefault(key, []).append(i)\n",
        "\n",
        "    # Sort line keys by top-y then left-x for correct reading order\n",
        "    def line_top(indices):\n",
        "        return min(data[\"top\"][i] for i in indices)\n",
        "    def line_left(indices):\n",
        "        return min(data[\"left\"][i] for i in indices)\n",
        "\n",
        "    sorted_keys = sorted(line_map.keys(),\n",
        "                         key=lambda k: (line_top(line_map[k]), line_left(line_map[k])))\n",
        "\n",
        "    units = []\n",
        "    for key in sorted_keys:\n",
        "        indices = line_map[key]\n",
        "        words = \" \".join(data[\"text\"][i] for i in indices)\n",
        "        if not words.strip():\n",
        "            continue\n",
        "        xs = [data[\"left\"][i]                     for i in indices]\n",
        "        ys = [data[\"top\"][i]                      for i in indices]\n",
        "        x2 = [data[\"left\"][i] + data[\"width\"][i]  for i in indices]\n",
        "        y2 = [data[\"top\"][i] + data[\"height\"][i]  for i in indices]\n",
        "        rect = pymupdf.Rect(\n",
        "            min(xs) * scale_x, min(ys) * scale_y,\n",
        "            max(x2) * scale_x, max(y2) * scale_y\n",
        "        )\n",
        "        units.append({\"text\": words, \"rect\": rect})\n",
        "\n",
        "    if not units:\n",
        "        return 0\n",
        "\n",
        "    original_texts   = [u[\"text\"] for u in units]\n",
        "    translated_texts = batch_translate(original_texts)\n",
        "\n",
        "    if use_verification:\n",
        "        translated_texts = verify_page_translations(\n",
        "            original_texts, translated_texts, batch_size=8\n",
        "        )\n",
        "\n",
        "    translated_texts = redact_spans(original_texts, translated_texts)\n",
        "\n",
        "    count = 0\n",
        "    for unit, translated_text in zip(units, translated_texts):\n",
        "        rect = unit[\"rect\"]\n",
        "        size = max(rect.height * 0.65 - 2.0, 5.0)\n",
        "\n",
        "        out_page.add_redact_annot(rect, fill=(1, 1, 1))\n",
        "        out_page.apply_redactions(images=pymupdf.PDF_REDACT_IMAGE_NONE)\n",
        "\n",
        "        exact_height = _compute_wrap_height(translated_text, size, rect.width, \"helv\")\n",
        "        expanded_rect = pymupdf.Rect(\n",
        "            rect.x0, rect.y0,\n",
        "            rect.x1, rect.y0 + exact_height\n",
        "        )\n",
        "\n",
        "        _wrap_and_insert(out_page, translated_text, expanded_rect,\n",
        "                         base_size=size, fontname=\"helv\", color=(0, 0, 0))\n",
        "        count += 1\n",
        "\n",
        "    return count\n",
        "\n",
        "\n",
        "print(\"‚úÖ Full generalized translation pipeline ready (digital + scanned)\")"
      ],
      "metadata": {
        "id": "XvilHOF04T3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 10: MAIN PIPELINE LOOP\n",
        "# Iterates every page, classifies it, routes to the correct\n",
        "# reconstruction function, and saves the translated PDF.\n",
        "# Also snapshots original English text for BLEU evaluation.\n",
        "# ============================================================\n",
        "\n",
        "# Snapshot original text before any modification (for BLEU)\n",
        "original_texts_for_bleu = []\n",
        "_snap = pymupdf.open(INPUT_PDF)\n",
        "for pg in range(len(_snap)):\n",
        "    original_texts_for_bleu.append(_snap[pg].get_text(\"text\"))\n",
        "_snap.close()\n",
        "\n",
        "src_doc = pymupdf.open(INPUT_PDF)\n",
        "out_doc = pymupdf.open()\n",
        "out_doc.insert_pdf(src_doc)\n",
        "src_doc.close()\n",
        "\n",
        "paddle_engine     = None\n",
        "digital_pages     = 0\n",
        "scanned_pages     = 0\n",
        "handwritten_pages = 0\n",
        "blank_pages       = 0\n",
        "skipped_pages     = 0\n",
        "\n",
        "for pg in range(len(out_doc)):\n",
        "    out_page = out_doc[pg]\n",
        "    info     = classify_page(out_page)\n",
        "\n",
        "    if info[\"page_type\"] == \"BLANK\":\n",
        "        print(f\"\\n‚¨ú Page {pg+1}/{len(out_doc)}: BLANK ‚Äî skipping\")\n",
        "        blank_pages += 1\n",
        "\n",
        "    elif info[\"page_type\"] == \"DIGITAL\":\n",
        "        print(f\"\\nüìÑ Page {pg+1}/{len(out_doc)}: DIGITAL \"\n",
        "              f\"({info['span_count']} spans, {info['drawings']} drawings)\")\n",
        "        count = reconstruct_digital_page(out_page)\n",
        "        print(f\"   ‚úÖ Translated & reinserted {count} spans\")\n",
        "        digital_pages += 1\n",
        "    elif info[\"is_scanned\"]:\n",
        "        if is_content_image(info):\n",
        "            print(f\"\\nüñºÔ∏è  Page {pg+1}/{len(out_doc)}: IMAGE-ONLY ‚Äî preserving as-is\")\n",
        "            skipped_pages += 1\n",
        "            continue\n",
        "        print(f\"\\nüì∑ Page {pg+1}/{len(out_doc)}: {info['page_type']} \"\n",
        "              f\"({info['span_count']} spans, {info['images']} images)\")\n",
        "        if PaddleOCR is not None:\n",
        "            if paddle_engine is None:\n",
        "                print(\"   ‚è≥ Initializing PaddleOCR...\")\n",
        "                paddle_engine = PaddleOCR(use_textline_orientation=True, lang=\"en\")\n",
        "            count = reconstruct_scanned_page_paddle(out_page, paddle_engine)\n",
        "            print(f\"   ‚úÖ PaddleOCR: translated {count} regions\")\n",
        "        else:\n",
        "            count = reconstruct_scanned_page_tesseract(out_page)\n",
        "            print(f\"   ‚úÖ Tesseract: translated {count} regions\")\n",
        "        scanned_pages += 1\n",
        "\n",
        "OUTPUT_PDF = os.path.join(WORK_DIR, \"courtaccess_translated.pdf\")\n",
        "out_doc.save(OUTPUT_PDF)\n",
        "out_doc.close()\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"üéâ PIPELINE COMPLETE\")\n",
        "print(f\"   Digital pages:      {digital_pages}\")\n",
        "print(f\"   Scanned pages:      {scanned_pages}\")\n",
        "print(f\"   Handwritten pages:  {handwritten_pages}\")\n",
        "print(f\"   Blank pages:        {blank_pages}\")\n",
        "print(f\"   Image-only skipped: {skipped_pages}\")\n",
        "print(f\"   Output: {OUTPUT_PDF}\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "files.download(OUTPUT_PDF)\n",
        "print(\"‚úÖ Downloaded: courtaccess_translated.pdf\")"
      ],
      "metadata": {
        "id": "2NN_OZ-N4Wx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 11: VISUAL COMPARISON & DOWNLOAD\n",
        "# Renders page 1 of original vs translated side-by-side for a\n",
        "# quick visual sanity check, then downloads the output PDF.\n",
        "# ============================================================\n",
        "\n",
        "def show_comparison(original_path, translated_path, page_num=0, dpi=120):\n",
        "    zoom = dpi / 72\n",
        "    mat  = pymupdf.Matrix(zoom, zoom)\n",
        "    orig_doc  = pymupdf.open(original_path)\n",
        "    trans_doc = pymupdf.open(translated_path)\n",
        "    pg        = min(page_num, len(orig_doc)-1, len(trans_doc)-1)\n",
        "\n",
        "    orig_pix  = orig_doc[pg].get_pixmap(matrix=mat)\n",
        "    trans_pix = trans_doc[pg].get_pixmap(matrix=mat)\n",
        "    orig_img  = Image.frombytes(\"RGB\", (orig_pix.width,  orig_pix.height),  orig_pix.samples)\n",
        "    trans_img = Image.frombytes(\"RGB\", (trans_pix.width, trans_pix.height), trans_pix.samples)\n",
        "\n",
        "    max_h  = max(orig_img.height, trans_img.height)\n",
        "    canvas = Image.new(\"RGB\", (orig_img.width + trans_img.width + 20, max_h), (200, 200, 200))\n",
        "    canvas.paste(orig_img,  (0, 0))\n",
        "    canvas.paste(trans_img, (orig_img.width + 20, 0))\n",
        "\n",
        "    draw = ImageDraw.Draw(canvas)\n",
        "    draw.text((10, 5),                      \"ORIGINAL (English)\",   fill=(255, 0, 0))\n",
        "    draw.text((orig_img.width + 30, 5),     \"TRANSLATED (Spanish)\", fill=(0, 128, 0))\n",
        "\n",
        "    out_path = os.path.join(WORK_DIR, \"comparison.png\")\n",
        "    canvas.save(out_path)\n",
        "    display(IPImage(out_path))\n",
        "    orig_doc.close()\n",
        "    trans_doc.close()\n",
        "\n",
        "print(\"üñºÔ∏è  Page 1 ‚Äî Original vs Translated:\")\n",
        "show_comparison(INPUT_PDF, OUTPUT_PDF, page_num=0)\n",
        "\n",
        "# files.download(OUTPUT_PDF)\n",
        "# print(\"‚úÖ Downloaded: courtaccess_translated.pdf\")"
      ],
      "metadata": {
        "id": "st6kHAWn4A9P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}